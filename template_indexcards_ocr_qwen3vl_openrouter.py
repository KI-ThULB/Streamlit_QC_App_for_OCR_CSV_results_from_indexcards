#!/usr/bin/env python3
"""
Museum und Archiv Karteikarten OCR mit Qwen VL
Multi-Batch-Verarbeitung f√ºr XX Ordner √† ~500 Karten
Optimiert f√ºr gro√üe Mengen (43.000+ Karten)

FIXES:
- ‚úÖ API-Endpoint korrigiert (chat/completions hinzugef√ºgt)
- ‚úÖ Error-Handling f√ºr API-Responses
- ‚úÖ bessere Logging-Informationen
"""

import os
import json
import base64
import requests
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import getpass
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import pickle
import glob

# === KONFIGURATION ===
# Hauptverzeichnis mit allen Batch-Ordnern
BASE_INPUT_DIR = "/XXXXXXXXXX"

# Muster f√ºr Batch-Ordner (anpassen falls n√∂tig)
BATCH_PATTERN = "*"  # oder "*" f√ºr alle Unterordner

# Ausgabeverzeichnisse
OUTPUT_BASE = "output_batches"
JSON_OUT_BASE = os.path.join(OUTPUT_BASE, "json")
CSV_OUT_BASE = os.path.join(OUTPUT_BASE, "csv")
FINAL_CSV = os.path.join(OUTPUT_BASE, "metadata_vlm_complete.csv")
LOG_FILE = os.path.join(OUTPUT_BASE, "vlm_errors.log")
CHECKPOINT_FILE = os.path.join(OUTPUT_BASE, "batch_checkpoint.pkl")
PROGRESS_FILE = os.path.join(OUTPUT_BASE, "batch_progress.json")

# API Konfiguration
API_BASE_URL = "https://openrouter.ai/api/v1"
API_ENDPOINT = f"{API_BASE_URL}/chat/completions"  # ‚úÖ FIXED: Vollst√§ndiger Endpoint

# W√§hle dein Modell:
MODEL_NAME = "qwen/qwen3-vl-8b-instruct"  # ‚úÖ Korrekt f√ºr OpenRouter

# Performance Einstellungen
MAX_WORKERS = 5              # Anzahl paralleler API-Aufrufe
MAX_RETRIES = 3              # Wiederholungen bei Fehlern
RETRY_DELAY = 2              # Sekunden zwischen Wiederholungen
BATCH_SIZE = 500             # Erwartete Anzahl Karten pro Batch

# Felder die extrahiert werden sollen
FIELD_KEYS = [
    "Komponist", "Signatur", "Titel", "Textanfang",
    "Verlag", "Material", "Textdichter", "Bearbeiter", "Bemerkungen" #bitte entsprechende Feldbezeichnungen auf Karte anpassen!
]

# Erstelle Verzeichnisstruktur
os.makedirs(JSON_OUT_BASE, exist_ok=True)
os.makedirs(CSV_OUT_BASE, exist_ok=True)

# Thread-safe Locks
stats_lock = Lock()
log_lock = Lock()

# === PROMPT F√úR STRUKTURIERTE EXTRAKTION - Bitte entsprechend anpassen ===
EXTRACTION_PROMPT = """Du bist ein Experte f√ºr die Digitalisierung historischer Archivkarteikarten. 

Analysiere diese Karteikarte aus dem XXXXXXX-Archiv und extrahiere ALLE vorhandenen Informationen in folgende Felder:

**WICHTIGE REGELN:**
1. Extrahiere EXAKT was auf der Karte steht, ohne zu interpretieren
2. Komponisten-Namen haben oft das Format "Nachname, Vorname" (z.B. "Zimmermann, Rolf")
3. Signaturen haben folgende Formate: 
   - Spez.XX.XXX (z.B. Spez.12.433)
   - Spez.XX.XXX [buchstabe] (z.B. Spez.16.734 w)
   - TOB XXXX (z.B. TOB 1728)
   - RTSO XXXX (z.B. RTSO 3953)
   - RTOB XXXX (z.B. RTOB 3891)
4. Wenn ein Feld leer ist, gib einen leeren String "" zur√ºck
5. Beachte die Labels auf der Karte: "Komponist:", "Titel:", "Signatur:", etc.
6. Bei handschriftlichem Text: bestm√∂gliche Transkription
7. Bei unleserlichen Stellen: markiere mit [unleserlich]

**FELDER:**
- Komponist: Name des Komponisten
- Signatur: Archiv-Signatur (siehe Formate oben)
- Titel: Titel des Musikst√ºcks
- Textanfang: Anfang des Liedtexts oder zus√§tzliche Informationen
- Verlag: Verlagsangabe
- Material: Art des Materials (z.B. "1 Part. u. Stimmen", "2 Part. u. Mater.")
- Textdichter: Name des Textdichters
- Bearbeiter: Name des Bearbeiters
- Bemerkungen: Zus√§tzliche Bemerkungen

**AUSGABEFORMAT:**
Antworte NUR mit einem validen JSON-Objekt (KEINE Markdown-Codebl√∂cke, KEINE Erkl√§rungen):

{
  "Komponist": "...",
  "Signatur": "...",
  "Titel": "...",
  "Textanfang": "...",
  "Verlag": "...",
  "Material": "...",
  "Textdichter": "...",
  "Bearbeiter": "...",
  "Bemerkungen": "..."
}
"""

# === HILFSFUNKTIONEN ===

def encode_image_to_base64(image_path):
    """Kodiert ein Bild als Base64-String."""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def call_vlm_api(image_path, api_key, max_retries=MAX_RETRIES):
    """
    Ruft das VLM API auf und gibt die strukturierten Daten zur√ºck.
    Mit automatischer Wiederholung bei Fehlern.
    
    ‚úÖ FIXED: Besseres Error-Handling f√ºr API-Responses
    """
    for attempt in range(max_retries):
        try:
            base64_image = encode_image_to_base64(image_path)
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
            
            payload = {
                "model": MODEL_NAME,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": EXTRACTION_PROMPT
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                "temperature": 0.1,
                "max_tokens": 1000
            }
            
            # ‚úÖ FIXED: Richtiger Endpoint
            response = requests.post(API_ENDPOINT, headers=headers, json=payload, timeout=120)
            
            # ‚úÖ FIXED: Besseres Error-Handling
            if response.status_code != 200:
                error_body = response.text
                try:
                    error_json = response.json()
                    error_msg = error_json.get("error", {}).get("message", error_body)
                except:
                    error_msg = error_body
                
                if response.status_code == 401:
                    raise Exception(f"API-Authentifizierung fehlgeschlagen: {error_msg}")
                elif response.status_code == 429:
                    raise Exception(f"Rate limit erreicht: {error_msg}")
                elif response.status_code == 400:
                    raise Exception(f"Falscher Request: {error_msg}")
                else:
                    raise Exception(f"API-Fehler ({response.status_code}): {error_msg}")
            
            result = response.json()
            
            if "choices" in result and len(result["choices"]) > 0:
                content = result["choices"][0]["message"]["content"]
                
                # Parse JSON aus der Antwort (bereinige Markdown)
                content = content.strip()
                if content.startswith("```json"):
                    content = content[7:]
                if content.startswith("```"):
                    content = content[3:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()
                
                data = json.loads(content)
                return data, None
            else:
                raise Exception("Keine 'choices' in API-Antwort erhalten")
                
        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:
            if attempt < max_retries - 1:
                print(f"     ‚ö†Ô∏è  Versuch {attempt + 1} fehlgeschlagen, Wiederholung in {RETRY_DELAY}s...")
                time.sleep(RETRY_DELAY * (attempt + 1))
                continue
            else:
                return None, str(e)
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"     ‚ö†Ô∏è  Versuch {attempt + 1} fehlgeschlagen, Wiederholung in {RETRY_DELAY}s...")
                time.sleep(RETRY_DELAY * (attempt + 1))
                continue
            return None, str(e)
    
    return None, "Max retries erreicht"

def log_error(batch_name, filename, message, details=None):
    """Schreibt Fehler in die Logdatei (thread-safe)."""
    with log_lock:
        with open(LOG_FILE, "a", encoding="utf-8") as log:
            log.write(f"[{datetime.now().isoformat()}] Batch: {batch_name} | Datei: {filename}\n")
            log.write(f"‚ö†Ô∏è  {message}\n")
            if details:
                log.write(f"Details: {details}\n")
            log.write("-" * 80 + "\n")

def validate_signature(signature):
    """Validiert ob eine Signatur ein g√ºltiges Format hat."""
    if not signature:
        return False
    
    import re
    patterns = [
        r'^Spez\.\d{1,2}\.\d{3,4}(\s+[a-z])?$', #Beispiele, wie die Signaturen und Nummernkreise aussehen k√∂nnen, bitte anpassen!
        r'^(RTSO|RTOB|TOB)\s+\d{3,4}$'
    ]
    
    return any(re.match(pattern, signature) for pattern in patterns)

def format_time(seconds):
    """Formatiert Sekunden in lesbares Format."""
    return str(timedelta(seconds=int(seconds)))

def save_checkpoint(data):
    """Speichert den aktuellen Fortschritt."""
    with open(CHECKPOINT_FILE, 'wb') as f:
        pickle.dump(data, f)

def load_checkpoint():
    """L√§dt den gespeicherten Fortschritt."""
    if os.path.exists(CHECKPOINT_FILE):
        try:
            with open(CHECKPOINT_FILE, 'rb') as f:
                return pickle.load(f)
        except:
            return {}
    return {}

def save_progress(progress):
    """Speichert Fortschritt in JSON."""
    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
        json.dump(progress, f, indent=2, ensure_ascii=False)

def load_progress():
    """L√§dt Fortschritt aus JSON."""
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}

# === WORKER FUNKTION ===

def process_single_card(image_path, api_key, batch_name):
    """Verarbeitet eine einzelne Karteikarte."""
    start_time = time.time()
    filename = image_path.name
    
    try:
        data, error = call_vlm_api(str(image_path), api_key)
        
        if error:
            log_error(batch_name, filename, error)
            return {
                "filename": filename,
                "batch": batch_name,
                "success": False,
                "error": error,
                "duration": time.time() - start_time
            }
        
        # F√ºge Metadaten hinzu
        data["Datei"] = filename
        data["Batch"] = batch_name
        
        # Speichere JSON (in batch-spezifischem Unterordner)
        batch_json_dir = Path(JSON_OUT_BASE) / batch_name
        batch_json_dir.mkdir(exist_ok=True)
        json_path = batch_json_dir / f"{image_path.stem}.json"
        
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return {
            "filename": filename,
            "batch": batch_name,
            "success": True,
            "data": data,
            "duration": time.time() - start_time,
            "has_komponist": bool(data.get("Komponist", "").strip()),
            "has_signatur": bool(data.get("Signatur", "").strip()),
            "valid_signatur": validate_signature(data.get("Signatur", ""))
        }
        
    except Exception as e:
        log_error(batch_name, filename, f"Unerwarteter Fehler: {str(e)}")
        return {
            "filename": filename,
            "batch": batch_name,
            "success": False,
            "error": str(e),
            "duration": time.time() - start_time
        }

# === BATCH-VERARBEITUNG ===

def process_single_batch(batch_dir, api_key, batch_number, total_batches):
    """Verarbeitet einen einzelnen Batch-Ordner."""
    
    batch_name = batch_dir.name
    print(f"\n{'=' * 80}")
    print(f"üì¶ BATCH {batch_number}/{total_batches}: {batch_name}")
    print(f"{'=' * 80}")
    
    # Lade Checkpoint f√ºr diesen Batch
    checkpoint = load_checkpoint()
    processed_files = checkpoint.get(batch_name, set())
    
    # Finde alle Bilder
    all_files = sorted(list(batch_dir.glob("*.jpg")) + list(batch_dir.glob("*.jpeg")))
    image_files = [f for f in all_files if f.name not in processed_files]
    
    total = len(image_files)
    already_processed = len(all_files) - total
    
    if already_processed > 0:
        print(f"üìå {already_processed} Karten bereits verarbeitet (wird fortgesetzt)")
    
    if total == 0:
        if already_processed > 0:
            print(f"‚úÖ Batch vollst√§ndig verarbeitet ({already_processed} Karten)")
        else:
            print(f"‚ö†Ô∏è  Keine Bilder gefunden")
        return None
    
    print(f"üìö Verarbeite {total} neue Karteikarten...")
    print(f"üîó API Endpoint: {API_ENDPOINT}")
    print(f"ü§ñ Modell: {MODEL_NAME}")
    
    # Statistiken
    records = []
    success_count = 0
    error_count = 0
    komponist_count = 0
    signatur_count = 0
    valid_signatur_count = 0
    
    batch_start = time.time()
    last_update = time.time()
    processed_count = 0
    
    # Parallele Verarbeitung
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_file = {
            executor.submit(process_single_card, img_path, api_key, batch_name): img_path 
            for img_path in image_files
        }
        
        for future in as_completed(future_to_file):
            result = future.result()
            processed_count += 1
            
            if result["success"]:
                success_count += 1
                records.append(result["data"])
                processed_files.add(result["filename"])
                
                if result.get("has_komponist"):
                    komponist_count += 1
                if result.get("has_signatur"):
                    signatur_count += 1
                if result.get("valid_signatur"):
                    valid_signatur_count += 1
            else:
                error_count += 1
            
            # Progress Update
            current_time = time.time()
            if current_time - last_update >= 5 or processed_count % 10 == 0:
                elapsed = current_time - batch_start
                avg_time = elapsed / processed_count
                remaining = total - processed_count
                eta_seconds = remaining * avg_time
                eta = format_time(eta_seconds)
                
                cards_per_min = (processed_count / elapsed) * 60 if elapsed > 0 else 0
                
                print(f"  üìä [{processed_count}/{total}] | "
                      f"‚úì {success_count} | ‚úó {error_count} | "
                      f"{cards_per_min:.1f}/min | "
                      f"ETA: {eta}")
                
                last_update = current_time
                
                # Checkpoint alle 50 Karten
                if processed_count % 50 == 0:
                    checkpoint[batch_name] = processed_files
                    save_checkpoint(checkpoint)
    
    # Batch-Statistiken
    batch_duration = time.time() - batch_start
    
    print(f"\n  ‚è±Ô∏è  Batch-Dauer: {format_time(batch_duration)}")
    print(f"  ‚ö° Durchschnitt: {batch_duration / total:.2f}s pro Karte")
    print(f"  ‚úÖ Erfolgreich: {success_count}/{total} ({success_count/total*100:.1f}%)")
    
    if success_count > 0:
        print(f"  üìù Komponisten: {komponist_count} ({komponist_count/success_count*100:.1f}%)") #kann je nach Auskunftstiefe angepasst werden auf Feldbezeichnungen
        print(f"  üîñ Signaturen: {signatur_count} ({signatur_count/success_count*100:.1f}%)")
    
    # Speichere Batch-CSV
    if records:
        df = pd.DataFrame(records)
        cols = ["Datei", "Batch", "Signatur", "Komponist"] + \
               [k for k in FIELD_KEYS if k not in ["Signatur", "Komponist"] and k in df.columns]
        df = df[cols]
        
        csv_filename = f"{batch_name}.csv"
        csv_path = os.path.join(CSV_OUT_BASE, csv_filename)
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        
        print(f"  üíæ CSV gespeichert: {csv_filename}")
        
        # Update Checkpoint
        checkpoint[batch_name] = processed_files
        save_checkpoint(checkpoint)
        
        return {
            "batch_name": batch_name,
            "total_cards": len(all_files),
            "processed": total,
            "success": success_count,
            "errors": error_count,
            "duration": batch_duration,
            "csv_file": csv_path,
            "komponist_found": komponist_count,
            "signatur_found": signatur_count,
            "valid_signatur": valid_signatur_count
        }
    
    return None

# === HAUPTPROGRAMM ===

def process_all_batches():
    """Verarbeitet alle Batch-Ordner."""
    
    print("üéµ Archiv Multi-Batch OCR") #je nach Institutionen bitte Name anpassen!
    print("=" * 80)
    print(f"ü§ñ Modell: {MODEL_NAME}")
    print(f"‚ö° Parallele Verarbeitung mit {MAX_WORKERS} Workern")
    print(f"üîó API Endpoint: {API_ENDPOINT}")
    print("=" * 80)
    
    # API-Key abfragen
    print("\nüîë Bitte gib deinen API-Key ein:")
    api_key = getpass.getpass("API-Key: ")
    
    if not api_key:
        print("‚ùå Kein API-Key angegeben. Abbruch.")
        return
    
    # L√∂sche alte Logs
    if os.path.exists(LOG_FILE):
        os.remove(LOG_FILE)
    
    # Finde alle Batch-Ordner
    base_path = Path(BASE_INPUT_DIR)
    
    # Methode 1: Suche nach Muster (z.B. "Batch_*")
    batch_dirs = sorted(list(base_path.glob(BATCH_PATTERN)))
    
    # Methode 2: Falls ALLE Unterordner Batches sind
    if not batch_dirs:
        batch_dirs = sorted([d for d in base_path.iterdir() if d.is_dir()])
    
    if not batch_dirs:
        print(f"‚ùå Keine Batch-Ordner gefunden in: {BASE_INPUT_DIR}")
        print(f"   Gesucht nach Muster: {BATCH_PATTERN}")
        return
    
    total_batches = len(batch_dirs)
    print(f"\nüì¶ {total_batches} Batch-Ordner gefunden")
    
    # Lade Fortschritt
    progress = load_progress()
    completed_batches = progress.get("completed_batches", [])
    
    # Verarbeite jeden Batch
    overall_start = time.time()
    batch_results = []
    
    for idx, batch_dir in enumerate(batch_dirs, 1):
        batch_name = batch_dir.name
        
        # √úberspringe bereits abgeschlossene Batches
        if batch_name in completed_batches:
            print(f"\n‚úÖ Batch {idx}/{total_batches}: {batch_name} (bereits abgeschlossen)")
            continue
        
        try:
            result = process_single_batch(batch_dir, api_key, idx, total_batches)
            
            if result:
                batch_results.append(result)
                completed_batches.append(batch_name)
                
                # Speichere Fortschritt
                progress["completed_batches"] = completed_batches
                progress["last_updated"] = datetime.now().isoformat()
                save_progress(progress)
                
        except KeyboardInterrupt:
            print("\n\n‚è∏Ô∏è  Verarbeitung durch Benutzer unterbrochen.")
            print("üíæ Fortschritt wurde gespeichert.")
            return
        except Exception as e:
            print(f"\n‚ùå Fehler bei Batch {batch_name}: {e}")
            log_error(batch_name, "BATCH", f"Kritischer Fehler: {e}")
            continue
    
    # === FINALE ZUSAMMENF√úHRUNG ===
    
    print(f"\n{'=' * 80}")
    print("üìä ZUSAMMENF√úHRUNG ALLER CSV-DATEIEN")
    print(f"{'=' * 80}")
    
    # Sammle alle Batch-CSVs
    csv_files = sorted(glob.glob(os.path.join(CSV_OUT_BASE, "*.csv")))
    
    if not csv_files:
        print("‚ùå Keine CSV-Dateien gefunden zum Zusammenf√ºhren")
        return
    
    print(f"üìù F√ºge {len(csv_files)} CSV-Dateien zusammen...")
    
    # Lade und kombiniere alle CSVs
    all_dfs = []
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file, encoding="utf-8-sig")
            all_dfs.append(df)
        except Exception as e:
            print(f"‚ö†Ô∏è  Fehler beim Laden von {csv_file}: {e}")
    
    if all_dfs:
        # Kombiniere alle DataFrames
        combined_df = pd.concat(all_dfs, ignore_index=True)
        combined_df.to_csv(FINAL_CSV, index=False, encoding="utf-8-sig")
        
        print(f"‚úÖ Gesamt-CSV erstellt: {FINAL_CSV}")
        print(f"   üìä Gesamt-Eintr√§ge: {len(combined_df):,}")
    
    # === FINALE STATISTIKEN ===
    
    total_elapsed = time.time() - overall_start
    
    print(f"\n{'=' * 80}")
    print("üéâ VERARBEITUNG ABGESCHLOSSEN")
    print(f"{'=' * 80}")
    print(f"‚è±Ô∏è  Gesamtdauer: {format_time(total_elapsed)}")
    print(f"üì¶ Verarbeitete Batches: {len(batch_results)}/{total_batches}")
    
    if batch_results:
        total_cards = sum(r["total_cards"] for r in batch_results)
        total_success = sum(r["success"] for r in batch_results)
        total_errors = sum(r["errors"] for r in batch_results)
        
        print(f"üìö Gesamt-Karteikarten: {total_cards:,}")
        print(f"‚úÖ Erfolgreich: {total_success:,} ({total_success/total_cards*100:.1f}%)")
        print(f"‚ùå Fehler: {total_errors:,}")
        print(f"‚ö° Durchschnitt: {total_elapsed / total_cards:.2f}s pro Karte")
        print(f"üöÄ Geschwindigkeit: {(total_cards / total_elapsed) * 3600:.0f} Karten/Stunde")
    
    print(f"\nüìÇ Ausgabeverzeichnis: {OUTPUT_BASE}/")
    print(f"   ‚îú‚îÄ‚îÄ csv/ ({len(csv_files)} Batch-CSVs)")
    print(f"   ‚îú‚îÄ‚îÄ json/ (JSON-Dateien nach Batch)")
    print(f"   ‚îî‚îÄ‚îÄ {os.path.basename(FINAL_CSV)} (Gesamt-CSV)")
    
    if total_errors > 0:
        print(f"\n‚ö†Ô∏è  Fehlerprotokoll: {LOG_FILE}")
    
    print(f"{'=' * 80}")
    
    # L√∂sche Checkpoint nach erfolgreichem Abschluss
    if len(completed_batches) == total_batches:
        if os.path.exists(CHECKPOINT_FILE):
            os.remove(CHECKPOINT_FILE)
        if os.path.exists(PROGRESS_FILE):
            os.remove(PROGRESS_FILE)
        print("‚úÖ Alle Batches erfolgreich verarbeitet!")

if __name__ == "__main__":
    try:
        process_all_batches()
    except KeyboardInterrupt:
        print("\n\n‚è∏Ô∏è  Verarbeitung abgebrochen durch Benutzer.")
        print("üíæ Fortschritt wurde gespeichert. Beim n√§chsten Start wird fortgesetzt.")
    except Exception as e:
        print(f"\n‚ùå Kritischer Fehler: {e}")
        import traceback
        traceback.print_exc()
